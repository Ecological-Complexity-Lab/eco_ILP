{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import numpy as np\n",
    "from tqdm import tqdm # Progress Bar\n",
    "\n",
    "from helper.networks_functions import load_networks, process_networks, export2csv\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the databases | (Brimacombe's data)\n",
    "networks = load_networks(\n",
    "    path = 'data/raw/networks/Brimacombe/topological_heterogeneity_upload/networks/*.txt', \n",
    "    ignored_communities = ['Microbiome', 'Plant-Ant', 'Food-Web', 'Multiples', 'Anemone-Fish', 'Legislature', 'Actor', \n",
    "                            'Journal', 'Baseball', 'Basketball', 'Chicago', 'Denver', 'Hockey', 'Minneapolis', 'San Francisco','Washington']\n",
    "    )\n",
    "\n",
    "# Load the databases | (Web Of Life)\n",
    "# networks_new = load_wol(WOL_path = 'data/networks/Web Of Life/**/*.csv', rename_communities, symbiotic_relationships, columns_rows_drop, ignored_communities, transpose_list)\n",
    "# networks.update(networks_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing and subsampling\n",
    "networks = process_networks(\n",
    "    networks,\n",
    "    minNetworkSize = 20, \n",
    "    maxNetworkSize = 1000, \n",
    "    minConnectance = 0.1, \n",
    "    frac_list = [0.8],  # [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    reps = 1, # experimental \n",
    "    weighted = False, # currently the code is supporting only unweighted\n",
    "    min_components = False, # experimental \n",
    "    # networks_drop_list, \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the data to csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export2csv(networks, output_dir = 'data/processed/networks/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save another set for sensitivity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing and subsampling: 100%|██████████| 713/713 [00:45<00:00, 15.59it/s]\n",
      "Exporting to csv: 100%|██████████| 538/538 [00:10<00:00, 53.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# Processing and subsampling\n",
    "networks_sensetivity = process_networks(\n",
    "    networks,\n",
    "    minNetworkSize = 20, \n",
    "    maxNetworkSize = 1000, \n",
    "    minConnectance = 0.1, \n",
    "    frac_list = [0.7,0.75,0.8,0.85,0.9,0.95],\n",
    "    )\n",
    "export2csv(networks_sensetivity, output_dir='data/processed/networks/sensitivity/', file_prefix='subsamples_sensitivity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save another set for connectance threshold analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing and subsampling:  80%|███████▉  | 569/713 [01:21<00:16,  8.71it/s]"
     ]
    }
   ],
   "source": [
    "# Processing and subsampling\n",
    "networks_filtered = process_networks(\n",
    "    networks,\n",
    "    minNetworkSize = 20, \n",
    "    maxNetworkSize = 1000, \n",
    "    minConnectance = 0.1, \n",
    "    frac_list = [0.8],\n",
    "    reverse_filters=True\n",
    "    )\n",
    "export2csv(networks_filtered, output_dir='data/processed/networks/filtering/', file_prefix='subsamples_filtered')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "additional subsamples with biased sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "networks_lowDegBias_sampling = process_networks(networks, frac_list=[0.8], degree_biased='low')\n",
    "networks_highDegBias_sampling = process_networks(networks, frac_list=[0.8], degree_biased='high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exporting to csv:   0%|          | 0/538 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exporting to csv: 100%|██████████| 538/538 [00:02<00:00, 190.25it/s]\n",
      "Exporting to csv: 100%|██████████| 538/538 [00:02<00:00, 192.21it/s]\n"
     ]
    }
   ],
   "source": [
    "# Export to CSV\n",
    "export2csv(networks_lowDegBias_sampling, output_dir='data/processed/networks/biased_sampling/', file_prefix='lowDegBiasSampling')\n",
    "export2csv(networks_highDegBias_sampling, output_dir='data/processed/networks/biased_sampling/', file_prefix='highDegBiasSampling')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
